---
title: "Linear Regression"
output:
  pdf_document: default
  html_notebook: default
---
By: Perry Son, Waheed Anwar


### Overview
Linear regression models a relationship between predictors and targets. Linear regression derives estimated coefficients of the predictors. By fitting a line, it minimizes the difference between estimated and actual output values. Linear regression supports simple interpretation, regularization, and incorporation of new data. However, it struggles with modeling non-linear relationships.

The data consists of house sales in King County. These homes are sold between 2014 and 2015. The dataset can be found at Kaggle(https://www.kaggle.com/datasets/harlfoxem/housesalesprediction).

### Loading Data
From the Kaggle website, I downloaded the csv file("kc_house_data.csv"). I loaded the csv file and printed the first 6 observations. 

```{r}
set.seed(1111)
current_path = rstudioapi::getActiveDocumentContext()$path 
setwd(dirname(current_path ))
house_data <- read.csv("kc_house_data.csv")
head(house_data)
```

I called the nrow function, which indicated there was 21613 observations. 
```{r}
nrow(house_data)
```

I printed the list of columns in the dataset.
```{r}
colnames(house_data)
```

I applied a NaN checking function to each column. No NaN's were detected in the dataset. 
```{r}
na_count <-sapply(house_data, function(y) sum(length(which(is.na(y)))))
na_count
```

With an 80/20 ratio, I split the data into training and test sets.
```{r}
i <- sample(1:nrow(house_data), nrow(house_data)*0.80, replace=FALSE)
train <- house_data[i,]
test <- house_data[-i,]
```

### Using R's Built-in Functions for Data Exploration

I applied the range function to find minimum and maximum house price. Price range from \$75000 to \$7700000.

```{r}
range(train$price)
```

There appears to be some correlation between price and sqft_living. Correlation is ~0.7.
```{r}
cor(train$price, train$sqft_living)
```
Average square feet of house lots is 15012.5. 
```{r}
mean(train$sqft_lot)
```

Median square feet of lot is 7614.5.
```{r}
median(train$sqft_lot)
```

Covariance between price and bedrooms is 104163.6.
```{r}
cov(train$price, train$bedrooms)
```
### Graphs of Training Data

A significant amount of houses were built from 2000 to 2010. Some houses are older than 100 years old. 
```{r}
hist(train$yr_built, col="lightblue", main="Year Built",
xlab="yr_built")
```


Median square feet of living is ~2000. Some outliers indicate that square feet of living can reach at least 10000. 
```{r}
boxplot(train$sqft_living, main="Square Feet of Living", horizontal = TRUE)
```


Most of the houses are classified as level 3. 
```{r}
barplot(table(train$condition), xlab="House Condition", ylab="Frequency",
col=c("red","blue","sienna3", "wheat", "black"))
```

## Constructing Linear Regression Models

For this notebook, I created 3 linear regression models. These models attempt to predict house price with various predictors. 

### Linear Model 1

Linear model (lm1) uses only one predictor(sqft_living) to predict house price. The summary function provides details on lm1's performance. The residual's range is [-1440641, 4393124 ]. This suggests that price diverges significantly as sqft_living increased. This suggests there are more variables that determine house price. With a low p-value, the predictor, sqft_living, has a potential relationship with house price. Estimate for sqft_living is 277.695. For every square foot of living space, house price increases by ~278 dollars. 

However, lm1 did not adequately fit the dataset. Residual standard error(RSE) has a high value of 258900. With an R<sup>2</sup> value of 0.49, the predictor can account for only 49% of variance in the model. 
```{r}
lm1 <- lm(price~sqft_living, data=train)
summary(lm1)
```
Regarding the Residuals vs Fitted graph, most of the residuals lie on top of the red trend line. This plot demonstrated that the residuals have non-linear patterns. Since the residuals are not equally distributed around the trend line, the model fails to capture this non-linear relationship.

The second graph, Normal Q-Q, indicates that most of the residuals are normally distributed except for those in the higher range. This means there are variations that lm1 failed to capture. 

The third graph, Scale-Location, shows the lack of equal variance. Residuals are not distributed equally around the red line. Instead, the residuals widely diverge along the x-axis.

The fourth graph, Residuals vs Leverage, identifies influential residuals in lm1. In this case, observation #7253 lies just outside of the Cook's distance. If we remove it, then this could significantly affect/improve the model's coefficients. 
```{r}
par(mfrow=c(2,2))
plot(lm1)
```


### Linear Model 2

Linear model 2 (lm2) uses multiple predictors to determine house price. It utilizes the columns: sqft_living, bedrooms, bathrooms, condition, yr_built, and floors. 

lm2 still has a wide range of residuals. Residuals lie between -1799550 and 3984918. This suggests that more predictors are required for analyzing price relationship. All of these predictors have low p-values. The one predictor, condition, has a slightly higher p-value. Various estimates have been assigned to the predictors' coefficients. 

Compared to lm1, lm2 does a slightly better job of fitting the training set. Its RSE is 241700. Its R<sup>2</sup> is 0.55. 
```{r}
lm2 <- lm(price~sqft_living+bedrooms+bathrooms+condition+yr_built+floors, data=train)
summary(lm2)
```

The Residuals vs Fitted graph still exhibits non-linear patterns. Most of the residuals are above the trend line. lm2 fails to model this non-linear relationship. 

The Normal Q-Q graph shows that most of the residuals are normally distributed except for those in the higher range. lm2 does not capture all of these variations. 

The Scale-Location graph shows the lack of equal variance. Residuals still vary along the x-axis.


The Residuals vs Leverage graph indicates that one outlier influenced the regression's coefficients. Most of the outliers are contained within the Cook's distance.


```{r}
par(mfrow=c(2,2))
plot(lm2)
```

### Linear Model 3

Linear model 3 (lm3) simulates a more complex relationship. It includes a new predictor(grade). It also incorporates interaction effects. Different predictors are multiplied with each other (ex: sqft_living * condition). 

lm3 had a wide range of residuals. Min and max values are -1988198 and 4369219. Interestingly, floors now has a high p-value (0.8483). That means it doesn't provide strong evidence of a possible effect.The rest of the predictors have low p-values. 

lm3 has a lower RSE (220800) than that of lm1 and lm2. It also fits the data slightly better. Its R<sup>2</sup> is 0.6315. It accounts for 63% of variation.  

```{r}
lm3 <- lm(price~sqft_living*condition+bedrooms*bathrooms*floors + yr_built*grade, data = train)
summary(lm3)
```


The Residuals vs Fitted graph exhibits slightly non-linear patterns. Most of the residuals are above the trend line. lm3 fails to model this non-linear relationship. 

The Normal Q-Q graph shows that most of the residuals are normally distributed except for those in the higher range. However, the residuals diverge farther along the x-axis. More of the residuals are showing characteristics of a normal distribution. lm3 captures more variation. 


The Scale-Location graph shows the lack of equal variance. Residuals still vary along the x-axis. 


The Residuals vs Leverage graph indicates that most of the observations are contained within the Cook Distance. That means more of the data gets accounted by the model. 


```{r}
par(mfrow=c(2,2))
plot(lm3)
```

### Comparison of Models

The following linear models had increasing levels of complexity. However, all of them had trouble modeling the data. Their residual plots indicate an inability to account for variance. lm1 used only 1 predictor, so it fails to model the relationship. It has high RSE and low R<sup>2</sup> values. lm2 uses multiple predictors. Compared to lm1, lm2 had a lower RSE and higher R^2. lm3 had the lowest RSE and highest R^2. Its residual plots slightly account for more variance. lm3 performed better than the rest of the linear models. lm3 had a more complex formula, which allowed it to account for some non-linear patterns. 


### Metrics Correlation and Evaluation

I listed the various correlation and MSE of the models. 

```{r}
pred <- predict(lm1, newdata = test)
corr1 <- cor(pred, test$price)
print(paste("correlation: ", corr1))
mse1 <- mean((pred - test$price)^2)
print(paste("mse: ", mse1))
```

```{r}
pred <- predict(lm2, newdata = test)
corr2 <- cor(pred, test$price)
print(paste("correlation: ", corr2))
mse2 <- mean((pred - test$price)^2)
print(paste("mse: ", mse2))
```

```{r}
pred <- predict(lm3, newdata = test)
corr3 <- cor(pred, test$price)
print(paste("correlation: ", corr3))
mse3 <- mean((pred - test$price)^2)
print(paste("mse: ", mse3))
```

### Comparison of Results

lm1 had the lowest correlation (70%). lm3 had the highest correlation (79%). lm3 accounted for more of the data's variation due to its complex formula. lm3 also has the lowest MSE. There are a lot of variables that determine house prices. Because they only have a few predictors and simple formulas, lm1 and lm2 had difficulty when modeling the data. Their MSE are higher than that of lm3. 


